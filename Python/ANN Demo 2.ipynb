{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507f2f15",
   "metadata": {},
   "source": [
    "## Forward Propagation in simple steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec7992",
   "metadata": {},
   "source": [
    "- Code for loading libraries has been skipped as of now. \n",
    "- I'm only focussing on how layers are created and activations calculated for subsequent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49036571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data\n",
    "\n",
    "#temperature and duration for roasting coffee beans\n",
    "xtrain = np.array([[200.0, 17]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043270c8",
   "metadata": {},
   "source": [
    "### Implementing forward prop one layer at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e07d9",
   "metadata": {},
   "source": [
    "The network defined here has only 2 layers- one hidden layer and one output layer.  \n",
    "Layer 1 has 3 neurons with sigmoid as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 1\n",
    "layer_1 = Dense(units = 3, activation = 'sigmoid')\n",
    "#calculate output of layer 1\n",
    "a1 = layer_1(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8761949",
   "metadata": {},
   "source": [
    "Layer 2 has 1 neuron with sigmoid as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 2\n",
    "layer_2 = Dense(units = 1, acitivation = 'sigmoid')\n",
    "a2 = layer_2(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87661fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making binary inference from output\n",
    "if a2 >= 0.5:\n",
    "    yhat = 1\n",
    "else:\n",
    "    yhat = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1f384",
   "metadata": {},
   "source": [
    "### Implementing forward propagation- short version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data\n",
    "#temperature and duration for coffee beans roasting\n",
    "x = np.array([[200.0, 17.0],\n",
    "              [120.0, 5.0],\n",
    "              [425.0, 20.0],\n",
    "              [212.0, 18.0]])\n",
    "#good coffee-bad coffee labels\n",
    "y = np.array([1,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375042ac",
   "metadata": {},
   "source": [
    "`Sequential()` strings all layers together in the given order.  \n",
    "In this way, we do not need to explicitly pass output of one layer to the next. This gets handled by Tensorflow itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define layers\n",
    "layer_1 = Dense(units = 3, activation = 'sigmoid')\n",
    "layer_2 = Dense(units = 1, activation = 'sigmoid')\n",
    "#create the mode\n",
    "model = Sequential([layer_1, layer_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ecc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A more compact version\n",
    "model = Sequential([\n",
    "    Dense(units = 3, activation = 'sigmoid', name = 'layer_1'),\n",
    "    Dense(units = 1, activation = 'sigmoid', name = 'layer_2')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8267fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile() defines a loss function and compile optimization\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x,y,            \n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "#make inference on new data\n",
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the updated weights\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46879a7e",
   "metadata": {},
   "source": [
    "### Steps in data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1dc485",
   "metadata": {},
   "source": [
    "#### Normalization layer\n",
    "\n",
    "The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:\n",
    "- create a \"Normalization Layer\". Note, this is not a layer in your model.\n",
    "- 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
    "- normalize the data.  \n",
    "It is important to apply normalization to any future data that utilizes the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60808556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create normalization layer\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "#learn the mean and variance of each column feature\n",
    "norm_l.adapt(X)  # learns mean, variance\n",
    "#normalize all column of the data\n",
    "Xn = norm_l(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a875fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting weights and biases\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b63278",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalizing test data and then making predictions\n",
    "X_test = np.array([\n",
    "    [200,13.9],  # postive example\n",
    "    [200,17]])   # negative example\n",
    "X_testn = norm_l(X_test)\n",
    "predictions = model.predict(X_testn)\n",
    "print(\"predictions = \\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b8467",
   "metadata": {},
   "source": [
    "To convert the probabilities to a decision, we apply a threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce834c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.zeros_like(predictions)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] >= 0.5:\n",
    "        yhat[i] = 1\n",
    "    else:\n",
    "        yhat[i] = 0\n",
    "print(f\"decisions = \\n{yhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dac10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compact version of above code chunk\n",
    "yhat = (predictions >= 0.5).astype(int)\n",
    "print(f\"decisions = \\n{yhat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
