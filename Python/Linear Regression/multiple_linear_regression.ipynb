{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f6813b",
   "metadata": {},
   "source": [
    "Implementation of Multiple Linear Regression\n",
    "\n",
    "Summary of steps:\n",
    "\n",
    "1. Create following functions:  \n",
    "- `compute_cost()` - returns the least squared error cost for training set.  \n",
    "- `compute_gradient()` - returns the partial derivative of all w parameters and b.    \n",
    "- `gradient_descent()` - runs gradient descent algo for given number of iterations and returns cost, parameter history along with final values of parameters w and b.    \n",
    "- `compute_model_output()` - will be used to visualize model performance. It takes in final values of w and b & returns output of our model for each training example.  \n",
    "        \n",
    "2. Plots created:  \n",
    "        - Scatter plot showing all training samples. [CHECK THIS ONE!! How to show all training samples on coordinate axes when multiple variables given?]  \n",
    "        - Cost vs. number of iterations keeping learning rate constant.  \n",
    "        - Cost vs. learning rate keeping number of iterations constant.  \n",
    "        - Display how the final regression line fits on our training data.  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f37e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#normalize function to normalize the dataframe variables.\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a0e6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8aeaf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def compute_cost(xtrain, ytrain, w, b):\n",
    "    \"\"\"\n",
    "    Objective- return the least squared error cost\n",
    "    Arguments-\n",
    "    xtrain [(m,n) matrix]- m training samples, n variables\n",
    "    ytrain [(n,) 1-D vector]- m target outputs\n",
    "    w [(n,) 1-D vector]- weight coefficients for n variables.\n",
    "    b - scalar parameter.\n",
    "    Returns-\n",
    "    Least squared error cost.\n",
    "    \"\"\"\n",
    "    cost = 0.0\n",
    "    m = xtrain.shape[0]\n",
    "    for i in range(m):\n",
    "        fx = np.dot(xtrain[i],w) + b\n",
    "        err = (fx - ytrain[i])**2\n",
    "        cost = cost + err\n",
    "    #average the sum of squared errors\n",
    "    cost = (1/2*m)*cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20c2c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute gradient\n",
    "def compute_gradient(xtrain, ytrain, w, b):\n",
    "    \"\"\"\n",
    "    Objective- returns partial derivate of the cost funtion w.r.t all parameters\n",
    "    Arguments-\n",
    "    xtrain [(m,n) matrix]- m training samples, n variables\n",
    "    ytrain [(n,) 1-D vector]- m target outputs\n",
    "    w [(n,) 1-D vector]- weight coefficients for n variables.\n",
    "    b - scalar parameter.\n",
    "    Returns-\n",
    "    dj_dw [(n,) 1-D vector]- gradient of all w parameters\n",
    "    dj_db [scalar]- gradient of parameter b.\n",
    "    \"\"\"\n",
    "    m,n = xtrain.shape\n",
    "    #1-D vector with n values; will store gradient of all w parameters\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        fx = np.dot(xtrain[i],w)+b\n",
    "        err = fx - ytrain[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err*xtrain[i][j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    return dj_dw, dj_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "669bf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the output of the linear regression model\n",
    "def compute_model_output(xtrain, w, b):\n",
    "    \"\"\"\n",
    "    Objective- compute the output of model given the values w and b\n",
    "    Arguments-\n",
    "    1. xtrain [(m,n) matrix]- m training samples, n variables\n",
    "    2. w [(n,) 1-D vector]- weight coefficients for n variables.\n",
    "    3. b - scalar parameter.\n",
    "    Returns-\n",
    "    fx [(m,) 1-D vector]- predicted output by model\n",
    "    \"\"\"\n",
    "    m = xtrain.shape[0]\n",
    "    #fx stores predicted outcomes for all m observations\n",
    "    fx = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        fx[i] = np.dot(xtrain[i],w) + b\n",
    "    \n",
    "    return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "200db754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that runs gradient descent\n",
    "def gradient_descent(xtrain, ytrain, w, b, alpha, iters, compute_cost, compute_gradient):\n",
    "    \"\"\"\n",
    "    Objective- runs the gradient descent algorithm for given number of iterations.\n",
    "    Arguments-\n",
    "    1. xtrain [(m,n) matrix]- m training samples, n variables\n",
    "    2. ytrain [(n,) 1-D vector]- m target outputs\n",
    "    3. w [(n,) 1-D vector]- weight coefficients for n variables.\n",
    "    4. b - scalar parameter.\n",
    "    5. alpha- learning rate\n",
    "    6. iters- total number of iterations.\n",
    "    7. compute_cost, compute_gradient- functions created earlier.\n",
    "    Returns:\n",
    "    1. J_history- stores iteration index & history of all cost values.\n",
    "    2. w_history, b_history- stores iteration index & all parameter values at each iteration.\n",
    "    3. w_in, b_in- final values of parameters w and b.\n",
    "    \"\"\"\n",
    "    w_in, b_in = w, b\n",
    "    J_history, w_history, b_history = [], [], []\n",
    "    m, n = xtrain.shape\n",
    "    for i in range(iters):\n",
    "        gradient_w, gradient_b = compute_gradient(xtrain, ytrain, w_in, b_in)\n",
    "        #update both parameters simultaneously.\n",
    "        w_in = w_in - alpha*gradient_w\n",
    "        b_in = b_in - alpha*gradient_b\n",
    "        \n",
    "        #prevent resource exhaustion\n",
    "        if i < 100000:\n",
    "            J_history.append([i,compute_cost(xtrain, ytrain, w, b)])\n",
    "            w_history.append([i,w_in])\n",
    "            b_history.append([i,b_in])\n",
    "        \n",
    "        #printing values at some iterations interval\n",
    "#         if (i % math.ceil(iters/10)) == 0:\n",
    "#             print(f\"Iteration: {i}, Cost: {J_history[i][1]:.3e}\", \n",
    "#                   f\"W: {w_in:.3e}, b: {b_in:.3e}\")\n",
    "        \n",
    "    return J_history, w_history, b_history, w_in, b_in\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6180262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##some initializations\n",
    "\n",
    "#dataset\n",
    "#area, bedroom, bathroom, number of floors\n",
    "xtrain = np.array([\n",
    "                   [7420, 4, 2, 3],\n",
    "                   [8960, 4, 4, 4],\n",
    "                   [6420, 3, 2, 2],\n",
    "                   [4320, 3, 1, 2],\n",
    "                   [3240, 4, 1, 3],\n",
    "                   [6615, 4, 2, 2],\n",
    "                   [4510, 4, 1, 2],\n",
    "                   [3630, 4, 1, 2],\n",
    "                   [8050, 2, 1, 1],\n",
    "                   [3000, 3, 1, 2],\n",
    "                   [3420, 5, 1, 2],\n",
    "                   [3850, 3, 1, 2]\n",
    "                  ])\n",
    "#price of the 12 houses\n",
    "ytrain = np.array([13300000, 12250000, 8855000, 8750000, 6107500, 6090000, 4480000, \n",
    "          3010000, 3003000, 2485000, 1960000, 1750000])\n",
    "\n",
    "#Feature scaling; Normalize variables.\n",
    "xtrain_normalized = normalize(xtrain)\n",
    "#ytrain is sent as a nested list to normalize() as it expects multiple samples.\n",
    "ytrain_normalized = normalize([ytrain])\n",
    "\n",
    "# #dimensions of training set\n",
    "m,n = xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09898fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlisting the nested list ytrain_normalized\n",
    "ytrain_normalized2 = []\n",
    "for sublist in ytrain_normalized:\n",
    "    for i in sublist:\n",
    "        ytrain_normalized2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b724d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "alpha = 0.01\n",
    "#total number of steps in gradient descent\n",
    "iters = 100000\n",
    "#initial values of parameters\n",
    "w = np.zeros(n)\n",
    "b = 0.0\n",
    "#call gradient descent\n",
    "J_history, w_history, b_history, final_w, final_b = gradient_descent(xtrain_normalized, ytrain_normalized2, w, b, alpha, iters, \n",
    "                                                                     compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcddc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the parameter values to analyse the performance on training set\n",
    "y_output = compute_model_output(xtrain_normalized, final_w, final_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
